{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_Train = x_train_image.reshape(60000, 784).astype('float32')\n",
    "x_Test = x_test_image.reshape(10000, 784).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_Train_normalize = x_Train / 255\n",
    "x_Test_normalize = x_Test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Train_OneHot = np_utils.to_categorical(y_train_label)\n",
    "y_Test_OneHot = np_utils.to_categorical(y_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "以上為預處理, 詳細說明可看 1.Mnist_Preprocess<br>\n",
    "接下來為建立模型\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "建立一個線性堆疊模型, 後續只要使用model.add()方法, 將各神經網路層加入模型即可\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "輸入層(input layer) => 隱藏層(Hidden layer) => 輸出層(Output layer)<br>\n",
    "units = 256: 定義\"隱藏層\"神經元個數256<br>\n",
    "input_dim = 784: 設定\"輸入層\"神經元個數784<br>\n",
    "kernel_initializer = 'normal': 使用常態分布的亂數, 初始化權重(weight)與偏差(bias)<br>\n",
    "activation = 'relu': 定義啟動函數為relu\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 256,\n",
    "                input_dim = 784,\n",
    "                kernel_initializer = 'normal',\n",
    "                activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "units = 10: 定義\"輸出層\"神經元個數10\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 10,\n",
    "                kernel_initializer = 'normal',\n",
    "                activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "隱藏層: 共256個神經元, 因為輸入層和隱藏層一起建立, 所以沒有顯示輸入層<br>\n",
    "輸出層: 共10個神經元\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 203,530\n",
      "Trainable params: 203,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "模型參數說明:<br>\n",
    "每一層Param是超參數(Hyper-Parameters), 我們需要透過反向傳播演算法, 更新神經元連結的權重與偏差<br>\n",
    "建立輸入層與隱藏層的公式: h1 = relu(X * W1 + b1)<br>\n",
    "建立輸出層的公式: y = softmax(h1 * W2 + b2)<br>\n",
    "所以每一層Param計算方式為: Param = (上一層神經元數量) * (本層的神經元數量) + (本層的神經元數量)<br>\n",
    "因此<br>\n",
    "200960 = 784 * 256 + 256<br>\n",
    "2570   = 256 * 10  + 10<br>\n",
    "全部必須訓練的超參數Trainable params是每一層的Param加總<br>\n",
    "203530 = 200960 + 2570<br>\n",
    "通常Trainable params數值越大, 代表此模型越複雜, 需要更多時間進行訓練\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "接下來使用compile方法對訓練模型進行設定<br>\n",
    "loss: 設定損失函數(loss function), 在深度學習中通常使用交叉熵(cross entropy), 訓練效果比較好<br>\n",
    "optimizer: 設定訓練時的最佳化方法, 在深度學習中使用adam最佳化方法, 可以讓訓練更快收斂, 並提高準確率<br>\n",
    "metric: 設定評估模型的方法是準確率(accurancy)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'adam', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.0262 - acc: 0.9938 - val_loss: 0.0812 - val_acc: 0.9758\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.0221 - acc: 0.9951 - val_loss: 0.0827 - val_acc: 0.9760\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.0184 - acc: 0.9958 - val_loss: 0.0784 - val_acc: 0.9768\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.0156 - acc: 0.9970 - val_loss: 0.0779 - val_acc: 0.9778\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.0128 - acc: 0.9978 - val_loss: 0.0791 - val_acc: 0.9778\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.0103 - acc: 0.9985 - val_loss: 0.0767 - val_acc: 0.9791\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.0089 - acc: 0.9988 - val_loss: 0.0801 - val_acc: 0.9773\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.0076 - acc: 0.9991 - val_loss: 0.0782 - val_acc: 0.9778\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.0069 - acc: 0.9990 - val_loss: 0.0820 - val_acc: 0.9782\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.0054 - acc: 0.9994 - val_loss: 0.0841 - val_acc: 0.9776\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(x = x_Train_normalize, y = y_Train_OneHot, validation_split = 0.2, epochs = 10, batch_size = 200, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
